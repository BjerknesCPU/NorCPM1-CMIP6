<chapter id="troubleshooting">
<title>Troubleshooting</title>

<!-- ======================================================================= -->
<sect1 id="troubleshooting_newcase">
<title>Troubleshooting create_newcase</title>

<para>
Generally, create_newcase errors are reported to the terminal and
should provide some guidance about what caused the error.
</para>

<para>
If create_newcase fails on a relatively generic error, first check
carefully that the command line arguments match the interfaces
specification.  Type
</para>

<screen>
> create_newcase -help
</screen>

<para>
and review usage.
</para>

</sect1>

<!-- ======================================================================= -->
<sect1 id="troubleshooting_configure">
<title>Troubleshooting configure</title>

<para>
Generally, configure errors are reported to the terminal and
should provide some guidance about what caused the error.
Most of this section deals with the "-case" option of configure
which is the biggest step in setting up a
case and the supporting input files and scripts.  The configure
step is fairly extensive and a brief description of what configure
does follows.  $CASEROOT is the top level case directory (i.e. the
location where the env xml files and the configure script is 
located for a specific case).
</para>

<para>
The first thing configure does is load the case environment variables.
In general, this is done by sourcing the $CASEROOT/Tools/ccsm_getenv.
For more information about the environment variables, see <xref linkend="faq_getenv"/>
</para>

<para>
Then the first major step for configure is to run the script $CASEROOT/Tools/generate_resolved.csh.
This cycles through each of the component template files in $CASEROOT/Tools/Templates
sequentially.  These component template files are copied from locations in the
component source code by create_newcase when the case is created.  Each component 
template file generates a component buildnml.csh and buildexe.csh script in $CASEROOT/Buildconf
based on the resolution, configuration, and other env settings.  Generally, an error in
this phase of configure will point to a specific component.  Begin by debugging the
component template file in $CASEROOT/Tools/Templates.  The component template filename
will be something like cam.cpl7.template for the cam component.  If there is a bug in 
the component template file, then it's probably important to fix the original copy of
the template file.  These can be found in the create_newcase scripts (i.e. search for
the string, template).
</para>

<para>
The specific implementation of the component template files is very much 
component dependent.  However, each must generate a buildnml.csh script in
the $CASEROOT/Buildconf directory to generate namelist input on-the-fly.
Each template file must generate a buildexe.csh script in the same
$CASEROOT/Buildconf directory to support the build of that component.
And each template file must support generation of input_data_list files
in the $CASEROOT/Buildconf directory either at the configure or build step to 
specify the required input files for that configuration and component.
</para>

<para>
Next, configure runs the $CASEROOT/Tools/generate_batch.csh script.  
This script generates the build and run scripts for the case.  This is
partly accomplished by running the mkbatch.$MACH script for the particular
machine.  That script is located in $CCSMROOT/scripts/ccsm_utils/Machines.
If there are problems with the resulting build or run script, the
error can usually be traced to the setup of the mkbatch.$MACH machine file.
</para>

<para>
For instance an error like this
</para>

<screen>
> create_newcase -case ~/cesm1/b40.B2000bad \
                 -res 0.23x0.31_0.23x0.31  \ 
                 -mach bluefire            \ 
                 -compset B 
> cd ~/cesm1/b40.B2000bad
> configure -case

Generating resolved namelist, prestage, and build scripts
build-namelist - No default value found for ncdata
user defined attributes:
key=ic_md val=00010101
Died at /user/ccsmroot/models/atm/cam/bld/build-namelist line 2019.
ERROR: generate_resolved.csh error for atm template
configure error: configure generated error in attempting to created resolved scripts
</screen>

<para>
indicates the generate_resolved.csh script failed in the atm template, which
is the cam template for this compset.  It also reports that the cam build-namelist step in the
cam template failed at line 2019.
In this case, CAM could not find a valid value of ncdata from its default_namelist.xml file.
To fix this particular problem, the user can supply an alternative initial dataset and the
update the value in either the <envar>CAM_CONFIG_OPTS</envar> values or in the
<filename>SourceMods/src.cam/user_nl_cam</filename>.
</para>

</sect1>

<!-- ======================================================================= -->
<sect1 id="troubleshooting_run_sub">
<title>Troubleshooting job submission problems</title>

<para>
This section addresses problems with job submission.  Most of the
problems associated with submission or launch are very site specific.
</para>
<para>
First, make sure the runscript, $CASE.$MACH.run, is submitted using the
correct batch job submission tool, whether that's qsub, bsub, or
something else, and for instance,  whether a redirection "<"
character is required or not.
</para>

<para>
Review the batch submission options being used.  These probably
appear at the top of the $CASE.$MACH.run script but also may be set
on the command line when submitting a job.  Confirm that the options
are consistent with the site specific batch environment, and that the
queue names, time limits, and hardware processor request makes sense
and is consistent with the case running.
</para>

<para>
Review the job launch command in the $CASE.$MACH.run script to make
sure it's consistent with the site specific recommended tool.  This
command is usually an mprun, mpiexec, aprun, or something similar.
It can be found just after the string "EXECUTION BEGINS HERE" in the
$CASE.$MACH.run script.
</para>

<para>
The batch and run aspects of the $CASE.$MACH.run script is setup by 
configure and uses a machine specific mkbatch.$MACH
script in the $CCSMROOT/scripts/ccsm_utils/Machines directory.  If the
run script is not producing correct batch scripts or job launching
commands, the mkbatch.$MACH script probably needs to be updated.
</para>

</sect1>

<!-- ======================================================================= -->
<sect1 id="troubleshooting_run_time">
<title>Troubleshooting runtime problems </title>

<para>
To check that a run completed successfully, check the last several lines of the cpl.log file
for the string " SUCCESSFUL TERMINATION OF CPL7-CCSM ".  A successful job also
usually copies the log files to the directory $CASEROOT/logs.  
</para>

<note><para>The first things to check if a job fails are whether the model
timed out, whether a disk quota limit was hit, whether a machine went down,
or whether a file system became full.  If any of those things happened, 
take appropriate corrective action and resubmit the job.
</para></note>

<para>
If it's not clear any of the above caused a case to fail, then there are 
several places to look for error messages in CESM1.
</para>

<itemizedlist spacing="compact">
<listitem><para>
Go the $RUNDIR directory.  This directory is set in the env_build.xml file.  This is 
the directory where CESM runs.  Each component writes its own log file, 
and there should be log files there for every component 
(i.e. of the form cpl.log.yymmdd-hhmmss).  Check each component log file for an error message,
especially at the end or near the end of each file.
</para></listitem>

<listitem><para>
Check for a standard out and/or standard error file in the $CASEROOT directory.  
The standard out/err file often captures
a significant amount of extra CESM output and it also often contains significant
system output when the job terminates.  Sometimes, a useful error message can be found well
above the bottom of a large standard out/err file.  Backtrack from the bottom in search 
of an error message.
</para></listitem>

<listitem><para>
Go the $RUNDIR directory.  Check for core files and review them
using an appropriate tool.
</para></listitem>

<listitem><para>
Check any automated email from the job about why a job failed.  This is sent by
the batch scheduler and is a site specific feature that may or may not exist.
</para></listitem>

<listitem><para>
Check the archive directory.  If a case failed, the log files or data may
still have been archived.  The archiver is turned on if DOUT_S is set to TRUE
in env_run.xml.  The archive directory is set by the env variable DOUT_S_ROOT
and the directory to check is $DOUT_S_ROOT/$CASE.
</para></listitem>
</itemizedlist>

<para>
A common error is for the job to time out which often produces minimal
error messages.  By reviewing the daily model date stamps in the cpl.log file and 
the time stamps of files in the $RUNDIR directory, there should be enough
information to deduce the start and stop time of a run.  If the model
was running fine, but the batch wall limit was reached, either reduce
the run length or increase the batch time limit request.  If the model
hangs and then times out, that's usually indicative of either a system
problem (an MPI or file system problem) or possibly a model problem.
If a system problem is suspected, try to resubmit the job to see if
an intermittent problem occurred.  Also send help to local site consultants 
to provide them with feedback about system problems and to get help.
</para>

<para>
Another error that can cause a timeout is a slow or intermittently
slow node.  The cpl.log file normally
outputs the time used for every model simulation day.  To review that
data, grep the cpl.log file for the string, tStamp
</para>

<screen>
> grep tStamp cpl.log.* | more
</screen>

<para>
which gives output that looks like this:
</para>

<screen>
tStamp_write: model date = 10120 0 wall clock = 2009-09-28 09:10:46 avg dt = 58.58 dt = 58.18
tStamp_write: model date = 10121 0 wall clock = 2009-09-28 09:12:32 avg dt = 60.10 dt = 105.90
</screen>

<para>
and review the run times for each
model day.  These are indicated at the end of each line.  The "avg dt = "
is the running average time to simulate a model day in the current
run and "dt = " is the time needed to simulate the latest model day.
The model date is printed in YYYYMMDD format and the wall clock
is the local date and time.  So in this
case 10120 is Jan 20, 0001, and it took 58 seconds to run that
day. The next day, Jan 21, took 105.90 seconds. If a wide
variation in the simulation time is observed for typical mid-month model
days, then that is suggestive of a system problem.  However, be
aware that there are variations in the cost of the CESM1 model over
time.  For instance, on the last day of every simulated month, 
CESM1 typically write netcdf files, and this can be a significant
intermittent cost.  Also, some models read data mid month or run
physics intermittently at a timestep longer than one day.  In those cases, some
run time variability would be observed and it would be caused by
CESM1, not system variability.  With system performance variability,
the time variation is typically quite erratic and unpredictable.
</para>

<para>
Sometimes when a job times out, or it overflows disk space, the
restart files will get mangled. With the exception of the CAM and CLM
history files, all the restart files have consistent sizes. Compare
the restart files against the sizes of a previous restart. If they
don't match, then remove them and move the previous restart into place
before resubmitting the job. Please see <link
linkend="running_ccsm_restarts">restarting a run</link>.
</para>

<para>
On HPC systems, it is not completely uncommon for nodes to fail or for access
to large file systems to hang. Please make sure a case fails
consistently in the same place before filing a bug report with CESM1.
</para>

</sect1>

</chapter>

<!-- ======================================================================= -->
<!-- FAQ =================================================================== -->
<!-- ======================================================================= -->

<chapter id="faq">
<title>Frequently Asked Questions (FAQ)</title>

<!-- ======================================================================= -->
<sect1 id="faq_casestuff">
<title>What are the directories and files in my case directory?</title>

<para>
The following describes many of the files and directories in the $CASEROOT directory.
</para>

<variablelist>

<varlistentry><term>Buildconf</term>
<listitem><para>
is the directory where the buildnml and buildexe component scripts
are generated by configure.  The input_data_list files are also 
generated by configure or the buildnml scripts and copied here.
</para></listitem>
</varlistentry>

<varlistentry><term>CaseDocs</term>
<listitem><para>
is a directory where copies of the latest namelist/text input files from the 
$RUNDIR are stored.  These exist only to help document the case setup and run.
</para></listitem>
</varlistentry>

<varlistentry><term>LockedFiles</term>
<listitem><para>
is the directory that holds copies of the locked files.
</para></listitem>
</varlistentry>

<varlistentry><term>MachinesHist</term>
<listitem><para>
is a directory where previous case configurations are stored.  In other
words, when configure -clean is run, the current configured scripts are
copied into this directory, so when configure -case is subsequently
run, there is an opportunity to review and compare previous setups.
</para></listitem>
</varlistentry>

<varlistentry><term>Macros.$MACH</term>
<listitem><para>
is the Makefile Macros file for the current configuration.  The Makefile
is located in the Tools directory and is identical on all machines.
The Macros file is a machine and compiler dependent file.  This file is
locked during the build step.
</para></listitem>
</varlistentry>

<varlistentry><term>README.case</term>
<listitem><para>
provides a summary of the commands used to generate this case.
</para></listitem>
</varlistentry>

<varlistentry><term>SourceMods</term>
<listitem><para>
contains directories for each component where case specific source
code modifications can be included.  The source files in these directories
will always be used in preference to the source code in CCSMROOT.  
This feature allows users to modify CESM source code on a case
by case basis if that preferable over making modifications in the
CCSMROOT sandbox.
</para></listitem>
</varlistentry>

<varlistentry><term>$CASE.$MACH.build</term>
<listitem><para>
is the script that is run interactively to build the CESM model.
</para></listitem>
</varlistentry>

<varlistentry><term>$CASE.$MACH.clean_build</term>
<listitem><para>
is the script that cleans the CESM build.
</para></listitem>
</varlistentry>

<varlistentry><term>$CASE.$MACH.l_archive</term>
<listitem><para>
is the script that is submitted to the batch queue to archive CESM data
to the long-term archive disk, like an hpss or mass storage system.
</para></listitem>
</varlistentry>

<varlistentry><term>$CASE.$MACH.run</term>
<listitem><para>
is the script that is submitted to the batch queue to run a CESM job.  This
script could also be run interactively if resources allow.
</para></listitem>
</varlistentry>

<varlistentry><term>check_input_data</term>
<listitem><para>
is a tool that checks for missing input datasets and provides a capability
for exporting them to local disk.
</para></listitem>
</varlistentry>

<varlistentry><term>configure</term>
<listitem><para>
is the script that is run to generate files in Buildconf and the 
build and run scripts for a case.
</para></listitem>
</varlistentry>

<varlistentry><term>create_production_test</term>
<listitem><para>
is a tool that generates an exact restart test in a separate directory
based on the current case.
</para></listitem>
</varlistentry>

<varlistentry><term>env_*.xml files</term>
<listitem><para>
contain variables used to setup, configure, build, and run CESM.
</para></listitem>
</varlistentry>

<varlistentry><term>logs</term>
<listitem><para>
is a directory that contains a copy of the component log files from 
successful case runs.
</para></listitem>
</varlistentry>

<varlistentry><term>timing</term>
<listitem><para>
is a directory that contains timing output from each successful case
run.
</para></listitem>
</varlistentry>

<varlistentry><term>xmlchange</term>
<listitem><para>
is a script that supports changing xml variables in the env files.
</para></listitem>
</varlistentry>

<varlistentry><term>$CASEROOT/Tools</term>
<listitem><para>
contains many scripts that are used to setup and configure the CESM
model as well as run it.  Some of particular note are
</para>
</listitem>
</varlistentry>
</variablelist>

<itemizedlist spacing="compact">
<listitem><para>
Makefile is the Makefile that will be used for the build.
</para></listitem>

<listitem><para>
Templates is a directory that contains all the component
template files used during the configure phase to generate buildnml and
buildexe scripts in the $CASEROOT/Buildconf directory.
</para></listitem>

<listitem><para>
ccsm_buildexe.csh
is invoked by $CASEROOT/$CASE.$MACH.build to generate the model executable.
This script calls the component buildexe scripts in Buildconf.
</para></listitem>

<listitem><para>
ccsm_buildnml.csh is invoked by 
$CASEROOT/$CASE.$MACH.build to generate the component namelists in $RUNROOT.
This script calls the component buildnml scripts in Buildconf.
</para></listitem>

<listitem><para>
ccsm_check_lockedfiles
checks that any files in the $CASEROOT/LockedFiles/ directory match
those in the $CASEROOT directory. This helps protect users from
overwriting variables that should not be changed.
</para></listitem>

<listitem><para>
ccsm_getenv 
converts the xml variables in $CASEROOT to csh environmental variables.
</para></listitem>

<listitem><para>
ccsm_l_archive.csh 
is the script that does long-term archiving of model data as part of the 
$CASE.$MACH.l_archive batch job.
</para></listitem>

<listitem><para>
ccsm_postrun.csh
may run the short & long-term
archivers, resubmit the run script, and run the timing script.
</para></listitem>

<listitem><para>
ccsm_prestage.csh
checks that required input datasets are available.
</para></listitem>

<listitem><para>
generate_batch.csh is the script that
generates resolved run and long-term archiving batch scripts by configure.
</para></listitem>

<listitem><para>
generate_resolved.csh
generates resolved buildnml and buildexe scripts in the
$CASEROOT/Buildconf directory for model components.
</para></listitem>

<listitem><para>
getTiming.csh generates the timing information.
</para></listitem>

<listitem><para>
getTiming.pl generates timing
information and is used by getTiming.csh.
</para></listitem>

<listitem><para>
mkDepends
generates Makefile dependencies in a form suitable for inclusion into a Makefile.
</para></listitem>

<listitem><para>
perf_summary.pl generates timing information.
</para></listitem>

<listitem><para>
st_archive.sh 
is the short-term archive script.  It moves model output out of run directory to
the short-term archive directory.  Associated with DOUT_S and DOUT_S_ROOT env
variables in env_run.xml.
</para></listitem>

<listitem><para>
taskmaker.pl derives pe counts and  task and thread geometry 
info based on env var values set in the env_mach_pes file.
</para></listitem>

<listitem><para>
xml2env 
converts env_*xml files to shell environment variable files that are
then sourced for inclusion in the model environment.  Used by the
ccsm_getenv script.
</para></listitem>

</itemizedlist>

</sect1>

<!-- ======================================================================= -->
<sect1 id="faq_getenv">
<title>What are CESM1 env variables and env xml files?</title>

<para>
CESM1 cases are configured largely through setting what CESM1 calls "environment 
variables".  These actually appear to the user as variables defined in xml
files.  Those files appear in the case directory once a case is created
and are named something like env_*.xml.  They are converted to actual environment
variables via a csh script called ccsm_getenv.  That script calls a perl script
called xml2env that converts the xml files to shell files that are then sourced
and removed.  The ccsm_getenv and xml2env exist in the $CASEROOT/Tools directory.
The environment variables are specified in xml files to support extra
automated error checking and automatic generation of env variable documentation.
If you want to have the ccsm environment variables in your local shell environment, do
the following
</para>

<screen>
> cd $CASEROOT
> source ./Tools/ccsm_getenv
</screen>

<para>
You must run the ccsm_getenv from the CASEROOT directory exactly as shown
above.  There are multiple env_*.xml files including env_case.xml, env_conf.xml,
env_mach_pes.xml, env_build.xml, and env_run.xml.  To a large degree, the
different env files exist so variables can be locked in different phases
of the case setup, build, and run process.  For more info on locking files,
see <xref linkend="faq_lockedfiles"/>.  The important point is that env_case.xml
variables cannot be changed after create_newcase is invoked.  env_conf and
env_mach_pes cannot be changed after configure is invoked unless you
plan to <link linkend="case_conf">reconfigure</link> the case.  env_build
variables cannot be changed after the model is built unless you plan to
<link linkend="rebuild_executable">clean and rebuild</link>.  env_run variables can be changed anytime.
The CESM1 scripting software checks that xml files are not changed when
they shouldn't be.
</para>

<para>
CESM recommends using the xmlchange tool to modify env variables.  This
will decrease the chance that typographical errors will creep into the
xml files.  Conversion of the xml files to environment variables can fail
silently with certain xml format errors.  To use xmlchange, do, for instance,
</para>

<screen>
> cd $CASEROOT
> ./xmlchange -file env_run.xml -id STOP_OPTION -val nmonths
> ./xmlchange -file env_run.xml -id STOP_N -val 6
</screen>

<para>
which will change the variables STOP_OPTION and STOP_N in the file env_run.xml
to the specified values.
The xml files can be edited manually, but users should take care not to
introduce any formatting errors that could lead to incomplete env settings.
If there appear to be problems with the env variables (i.e. if the
model doesn't seem to have consistent values compared to what's set in the xml files),
then confirm that the env variables are being set properly.  There are a couple
of ways to do that.  First, run the ccsm_getenv script as indicated above
and review the output generated by the command "env".  The env variables
should match the xml settings.  Another option is to edit the $CASEROOT/Tools/ccsm_getenv
script and comment out the line "rm $i:r".  That should leave the shell
env files around, and they can then be reviewed.  The latter approach 
should be undone as soon as possible to avoid problems running ccsm_getenv
later.
</para>

</sect1>

<!-- ======================================================================= -->
<sect1 id="faq_xmlchange">
<title>How do I modify the value of CESM env variables?</title>

<para>
CESM recommends using the xmlchange tool to modify env variables.  xmlchange
supports error checking as part of the implementation.  Also, using xmlchange
will decrease the chance that typographical errors will creep into the
xml files.  Conversion of the xml files to environment variables can fail
silently with certain xml format errors.  To use xmlchange, do, for instance,
</para>

<screen>
> cd $CASEROOT
> ./xmlchange -file env_run.xml -id STOP_OPTION -val nmonths
> ./xmlchange -file env_run.xml -id STOP_N -val 6
</screen>

<para>
which will change the variables STOP_OPTION and STOP_N in the file env_run.xml
to the specified values.
The xml files can be edited manually, but users should take care not to
introduce any formatting errors that could lead to incomplete env settings.
See also <link linkend="modifying_xml">.</link>  
</para>

</sect1>

<!-- ======================================================================= -->

<sect1 id="faq_badenvars">
<title>Why aren't my env variable changes working?</title>

<para>
It's possible that a formatting error has been introduced in the 
env xml files.  This would lead to problems in setting the env variables.
If there appear to be problems with the env variables (i.e. if the
model doesn't seem to have consistent values compared to what's set in the xml files),
then confirm that the env variables are being set properly.  There are a couple
of ways to do that.  First, run the ccsm_getenv script via
</para>
<screen>
> cd $CASEROOT
> source ./Tools/ccsm_getenv
> env
</screen>
<para>
and review the output generated by the command "env".  The env variables
should match the xml settings.  Another option is to edit the $CASEROOT/Tools/ccsm_getenv
script and comment out the line "rm $i:r".  That should leave the shell
env files around, and they can then be reviewed.  The latter approach 
should be undone as soon as possible to avoid problems running ccsm_getenv
later.
</para>
</sect1>

<!-- ======================================================================= -->
<sect1 id="faq_lockedfiles">
<title>Why is there file locking and how does it work?</title>

<para>
In CESM, there are several different env xml files.  These include
env_case.xml, env_conf.xml, env_mach_pes.xml, env_build.xml, and env_run.xml.  
These are organized so variables can be locked during different phases
of the case configuration, build, and run.  Locking variables is a feature
of CESM that prevents users from changing variables after they have been
resolved (used) in other parts of the scripts system.  The variables in
env_case are locked when create_newcase is called.  The env_conf and env_mach_pes
variables are locked when configure is called.  The env_build variables
are locked when CESM is built, and the env_run variables are never locked
and can be changed anytime.  In addition, the Macros file is locked as part
of the build step.  The $CASEROOT/LockedFiles directory saves copies of
the xml files to facilitate the locking feature.  In summary
</para>

<itemizedlist spacing="compact">
<listitem> <para>
&env_case.xml; is locked upon invoking &create_newcase; and
cannot be unlocked.  To change settings in env_case, a new case
has to be generated with <link linkend="creating_a_case">create_newcase</link>.
</para></listitem>
<listitem> <para>
&env_conf.xml; and &env_mach_pes.xml; are locked after running configure -case.
After changing variable values in these files, 
<link linkend="case_conf">reconfigure</link> the model using "configure
-cleanall" (or some variation) and then "configure -case".
</para></listitem>
<listitem> <para>
<filename>Macros.$MACH</filename> and &env_build.xml; are locked upon the
<emphasis>successful</emphasis> completion of
<command>$CASE.MACH.build</command>. Both <filename>Macros.$MACH</filename>
and &env_build.xml; can be unlocked by invoking
<command>$CASE.$MACH.cleanbuild</command> and then the model should be
<link linkend="rebuild_executable">rebuilt</link>.
</para></listitem>
</itemizedlist>

</sect1>

<!-- ======================================================================= -->
<sect1 id="faq_pelayout">
<title>How do I change processor counts and component layouts on processors? </title>

<para>
See <xref linkend="case_conf_setting_pes"/> or the use case
<xref linkend="use_case_pelayout"/>.
</para>

</sect1>

<!-- ======================================================================= -->
<sect1 id="faq_pio">
<title>What is pio? </title>

<para>
The parallel IO (PIO) library is included with CESM1 and is automatically built
as part of the CESM build.  Several CESM1 components
use the PIO library to read and/or write data.  The PIO library is
a set of interfaces that support serial netcdf, parallel
netcdf, or binary IO transparently.  The implementation allows
users to easily configure the pio setup on the fly to change
the method (serial netcdf, parallel netcdf, or binary data) as well as various 
parameters associated with PIO to optimize IO performance.
The pio parameter settings are described in detail in <xref linkend="running_ccsm_env_output"/>.
</para>

<para>
CESM1 prefers that data be written in CF compliant netcdf format
to a single file that is independent of all parallel decomposition
information.  Historically, data was written by gathering global 
arrays on a root processor and then writing the data from the root 
processor to an external file using serial netcdf. The reverse process
(read and scatter) was done for reading data.  This method is relatively 
robust but is not memory scalable, performance scalable, or performance
flexible.  This lead to the introduction of the PIO parallel IO library.
</para>
<para>
PIO works as follows.  The PIO library is initialized and information
is provided about the method (serial netcdf, parallel netcdf, or
binary data), and the number of desired IO processors and their layout.
The IO parameters define the set of processors that are involved in
the IO.  This can be as few as one and as many as all processors.
The data, data name and data decomposition are also provided to PIO.
Data is written through the PIO interface in the model specific
decomposition.  Inside PIO, the data is rearranged into a "stride 1"
decomposition on the IO processors and the data is then written
serially using netcdf or in parallel using pnetcdf or netcdf4/hdf5.
</para>
<para>
There are several benefits associated with using PIO.  First,
even with serial netcdf, the memory use can be significantly decreased
because the global arrays are decomposed across the IO processors
and written in chunks serially.  This is critical as CESM1 runs at higher
resolutions where global arrays need to be minimized due to memory
availability.  Second, pnetcdf can be turned on transparently
potentially improving the IO performance.  Third, PIO parameters
such as the number of IO tasks and their layout can be tuned to
reduce memory and optimize performance on a machine by machine basis.
Fourth, the standard global gather and write or read and global
scatter can be recovered by setting the number of io tasks to 1
and using serial netcdf.
</para>
<para>
CESM1 uses the serial netcdf implementation of PIO and
pnetcdf is turned off in PIO by default.  
To use pnetcdf, a pnetcdf library (like netcdf) must be available
on the local machine and PIO pnetcdf support must be turned on
when PIO is built.  This is done as follows
</para>
<procedure>
<step>
<para>
Locate the local copy of pnetcdf.  It must be version 1.1.1 or newer
library
</para>
</step>

<step>
<para>
Set LIB_PNETCDF in the Macros file to the directory of the pnetcdf
library (eg. /contrib/pnetcdf1.1.1/lib).
</para>
</step>


<step>
<para>
Add PNETCDF_PIO to the pio CONFIG_ARGS variable in the Macros file,
and set it to the directory of the top level of a standard pnetcdf 
installation (eg /contrib/pnetcdf1.1.1). 
</para>
</step>


<step>
<para>
Run the clean_build script if the model has already been built.
</para>
</step>


<step>
<para>
Run the build script to rebuilt pio and the full CESM1 system.
</para>
</step>


<step>
<para>
Change  IO namelist settings to pnetcdf and set appropriate
IO tasks and layout.
</para>
</step>

</procedure>                       

<para>
The PNETCDF_PIO variable tells pio to build with pnetcdf support turned
on.  The LIB_PNETCDF variable tells the CESM Makefile to link in the
pnetcdf library at the link step of the CESM1 build.
</para>

<para>
There is an ongoing effort between CESM, pio developers, pnetcdf
developers and hardware vendors to understand and improve the IO 
performance in the various library layers.  To learn more about pio, see
<ulink url="http://code.google.com/p/parallelio">http://code.google.com/p/parallelio.</ulink> 
</para>


</sect1>

<!-- ======================================================================= -->
<sect1 id="faq_pnetcdf">
<title>How do I use pnetcdf? </title>

<para>
See <xref linkend="faq_pio"/> 
</para>

</sect1>

<!-- ======================================================================= -->
<sect1 id="faq_morecoupler">
<title>Is there more information about the coupler/driver implementation? </title>

<para>
Additional implementation details are provided in the 
the <ulink url="../../cpl7/">CESM coupler user guide</ulink> 
about sequencing, parallel IO, performance, grids, threading, budgets, and other
items.
</para>

</sect1>

<!-- ======================================================================= -->
<sect1 id="faq_createowncompset">
<title>How do I create my own compset? </title>

<para>
Several compsets are hardwired in the CESM1 release.  "create_newcase -l" 
provides a current listing of supported "out-of-the-box" compsets. 
</para>

<para>
To create a customized compset,
</para>

<screen>
> cd $CCSMROOT/scripts
</screen> 

<para>Now copy sample_compset_file.xml to another file, e.g. my_compset.xml.</para> 

<screen>
> cp sample_compset_file.xml my_compset.xml
</screen>

<para>Edit the file, my_compset.xml, to create your own compset configuration.
In particular, the NAME, SHORTNAME, DESC, and COMP_ variables 
should be specified. The STATUS and CCSM_CCOST variables can be ignored.
Note: Other CESM env variables can also be added here.  See 
scripts/ccsm_utils/Case.template/config_compsets.xml for other variables 
that might be related to compset configuration.
</para> 
<para>Next run create_newcase with the optional -compset_file argument.</para>
<screen>
> create_newcase -case mycase -res f19_g16 -compset MYCS -mach mymach -compset_file my_compset.xml
</screen>
<para>The case <filename>mycase</filename> should have been generated and the configuration 
should be consistent with settings from the my_compset.xml file.</para> 


</sect1>

<!-- ======================================================================= -->
<sect1 id="faq_addgrid">
<title>How do I add a new grid? </title>

<para>
Support for several grids are hardwired in the CESM1 release.  "create_newcase -l" 
provides a current listing of supported "out-of-the-box" grids.   In general, CESM grids
are associated with a specific combination of atmosphere, land, and ocean/ice grids
using a particular naming convention like f19_g16 for the f19 atm/lnd grid combined
with the g16 ocn/ice grid.  The f19 atm/lnd grid is the shortname for the 1.9x2.5
finite volume dycore grid.  The g16 ocn/ice grid is the shortname for the gx1v6
one-degree displaced pole grid.  To add a new grid, just a few things need to be
done.  First, create both an individual long and shortname for the individual
grid (ie. f19=1.9x2.5) and for the grid combination (ie. f19_g16).  Then add that
grid to the config_grid.xml file.   Next, update individual components to support
those new grids.  This usually begins with the components template file, but may
also require source code modifications in some cases.  More details are provided 
below.
</para>

<para>
The most difficult issue associated with adding a new grid is generating the
required grid files for components and generating any new mapping files required
by the coupler.  On some level, the best advice is to mimic datasets that already
exist for other grids for your grid.  That means, produce new datasets that
follow the formatting standard.  CESM has some tools to help with this, and they
should be outlined in each components' documentation.  There is specific process for generating
mapping file in CESM.  To learn more about this process, contact CESM directly.
</para>

<para>
To add a new grid, edit the <filename>scripts/ccsm_utils/config_grid.xml</filename>
and add the specific individual grids in the first section with their nx and ny sizes.
The add the grid combination as a section below specifying the individual grids for
each component, the mapping filenames, and other information.  Just follow an example
of an existing grid definition.  Once this is complete, the grid will be "supported"
in the CCSM scripts and cases can be created with it.
</para>

<para>
The next step is to add support for the new grids in the components.  This would
generally be done initially in the component template files, <filename>models/*/*/bld/*.template
</filename>.  Each component handles this differently, so it's up to the user to
decide which components need updates and how to best implement the update.  As part
of the template update, new grid files and input datasets may have been generated.
These need to be added to each component on an individual basis.
</para>

</sect1>

<!-- ======================================================================= -->

<sect1 id="faq_calendars">
<title>What calendars are supported in CESM? </title>

<para>
In general, the only supported calendar in the CESM time manager 
is the 365 day or no-leap calendar.  This calendar has the standard
12 months, but it has 365 days every year and 28 days in every February.  
Monthly averages in CESM are truly computed over varying number of days
depending on the month of the year.  
</para>
<para>
A gregorian calendar is available if the ESMF time manager is used.
To use the ESMF time manager, the ESMF library has to be available
locally and the ESMF library has to be turned on in CESM.  See
<link linkend="use_case_esmfint">using ESMF in CESM</link> for more
information about setting up CESM with the ESMF library.
</para>

</sect1>

<!-- ======================================================================= -->
<sect1 id="faq_addcomponent">
<title>How do I add a new component model to CESM1? </title>

<para>
Support for specific components (ie. cam, pop, cice, clm, datm, etc) is hardwired
into the CESM1 scripts.  To add a new component model, specifically a new
atmosphere, land, ocean, or sea ice component, several things need to be done.
This section only summarizes the tasks to complete without providing details.
This is an advanced exercise, and CESM may be able to provide addition assistance
if resources are available.  In the directions below, the model "cxyz" is a new
land model that is going to be added to CCSM.  There are two major parts.  First,
the component needs to be supported in the CESM scripts.  Second, the component
needs to be able to run under the CESM driver.
</para>


<itemizedlist spacing="compact">

<listitem><para>
Add the new model under the appropriate models/"component" directory.  For instance
add the cxyz model under models/lnd/cxyz.  
</para></listitem>

<listitem><para>
Add a template file to the new model under the bld directory.  For instance, 
models/lnd/cxyz/bld/cxyz.cpl7.template.  Use another models' template file as a
starting point.  The template file should be able to produce buildnml, buildexe,
and input_data_list files in the Buildconf directory.
</para></listitem>

<listitem><para>
Edit scripts/create_newcase.  Add support for the new model under the definition
of "my @comps".  Also add a pointer to the template file under the variable
"my %templates".
</para></listitem>

<listitem><para>
Add the new model as a valid option in the scripts/ccsm_utils/Case.template/config_definition.xml
file for the specific component.  For instance, for COMP_LND, add cxyz as a valid option. 
Add any new env variables that are needed specifically for that component and create a new
group for them called, for instance, "conf_cxyz".
Add the new model to the list of components in scripts/ccsm_utils/Case.template/ConfigCase.pm
if any component specific env variables are added to the config_definition.xml file.
</para></listitem>

<listitem><para>
Add a new compset that supports the new component to scripts/ccsm_utils/Case.template/config_compsets.xml.  For instance, add a new compset called IZ that is based on the I_2000 compset but has cxyz instead
of clm as the land component.
</para></listitem>

<listitem><para>
Review the Macros files in scripts/ccsm_utils/Machines to provide any compiler modifications
specifically for the new component.  This might be a step that is done after further testing.
</para></listitem>

<listitem><para>
Add support for output files for the new component in the scripts/ccsm_utils/Tools/st_archive.sh
script.  This might be done after the new component is in "production".
</para></listitem>

<listitem><para>
For the new component to run under the CCSM driver, a new top level *_comp_mct.F90 file needs
to be added to the component.  For instance, a lnd_comp_mct.F90 file should exist in the
cxyz model source code.  This file will provide the init, run, and final interfaces
that the CESM driver require.  To generate one of these for the new component, copy an
existing one from another component and modify it to run with the new component.  There
are several inherent requirements that make this work such as
<itemizedlist spacing="compact">
<listitem><para> the driver provides the mpi communicator for the component at initialization. 
The component must save and use this mpi communicator internally.</para></listitem>
<listitem><para>The top level "program" file for the new component should be disabled.  </para></listitem>
<listitem><para> the component must set various parameters at initialization such as
the present and prognostic flags, and the nx and ny size values.</para></listitem>
<listitem><para> the component must pass the grid and decomposition to the driver at
initialization in the particular format required.</para></listitem>
<listitem><para> the component must unpack and pack data from the coupling datatype
at initialization and during runtime.
the fields used must be set in the seq_flds_mod.F90 module in the driver. </para></listitem>
<listitem><para> the component must stay synchronized in time with the provided driver
time and should abort if time coordination is incorrect.  the component must advance
the correct amount of time whenever the run method is called.  the time is provided
to the component by the driver.
</para></listitem>
</itemizedlist>
</para></listitem>

</itemizedlist>

</sect1>

<!-- ======================================================================= -->
<sect1 id="faq_cice_and_pop_decomps">
<title>How are cice and pop decompositions set and how do I override them? </title>

<para>
The pop and cice models both have similar decompositions
and strategies for specifying the decomposition.  Both
models support decomposition of the horizontal grid into
two-dimensional blocks, and these blocks are then allocated
to individual processors inside each component.  The
decomposition must be specified when the models are built.
There are four environment variables in env_build.xml for each model that specify the
decomposition used.  These variables are POP or CICE followed
by _BLCKX, _BLCKY, _MXBLCKS, and _DECOMP.  BLCKX and BLCKY
specify the size of the local block in grid cells in the
"x" and "y" direction.  MXBLCKS specifies the maximum
number of blocks that might be on any given processor,
and DECOMP specifies the strategy for laying out the blocks
on processors.
</para>

<para>
The values for these environment variables are set automatically
by scripts in the cice and pop "bld" directories when
"configure -case" is run.  The scripts that generate
the decompositions are
</para>

<screen>
models/ocn/pop2/bld/generate_pop_decomp.pl
models/ice/cice/bld/generate_cice_decomp.pl
</screen>
<para>
Those tools leverage decompositions stored in xml files,
</para>
<screen>
models/ocn/pop2/bld/pop_decomp.xml
models/ice/cice/bld/cice_decomp.xml
</screen>

<para>
to set the decomposition for a given resolution and
total processor count.  The decomposition used can have
a significant effect on the model performance, and
the decompositions specified by the tools above generally
provide optimum or near optimum values for the given
resolution and processor count.  More information about
cice and pop decompositions can be found in each of those
user guides.
</para>

<para>
The decompositions can be specified manually by setting the
environment variable POP_AUTO_DECOMP or CICE_AUTO_DECOMP to
false in env_mach_pes.xml (which turns off use of the
scripts above) and then setting the four BLCKX,
BLCKY, MXBLCKS, and DECOMP environment variables in env_build.xml.
</para>

<para>
In general, relatively square and evenly divided
Cartesian decompositions work well for pop at low to
moderate resolution.  Cice performs best with "tall
and narrow" blocks because of the load imbalance
for most global grids between the low and high
latitudes.  At high resolutions, more than one block
per processor can result in land block elimination
and non-Cartesian decompositions sometimes perform better.
Testing of several decompositions is always
recommended for performance and validation before a long
run is started.
</para>
</sect1>
<!-- ======================================================================= -->
<sect1 id="faq_history_file_output_frequency">
<title>How do I change history file output frequency and content for CAM and CLM during a run? </title>
<para>
If you want to change the frequency of output for CAM or CLM (i.e. generate output every 6 model hours instead of once a model day) in the middle of a run, or if you want to change the fields that are output, in the middle of a run,  you need to stop the run, rebuild and rerun it with the same casename and branch from the same casename.  See the steps below for doing a branch run while retaining the casename. </para> 
<para> 
Rebuilding the case and restarting it where you left off, are necessary because CAM and CLM only read namelist variables once, at the beginning of a run.  
This is not the case for POP and CICE, they read the namelist input on every restart, and therefore for POP and CICE, you can change output fields and frequency by modifying the appropriate namelist variables and then doing a restart. 
</para>
<para>
The following example shows case B40.20th.1deg which runs from 1850 to 2005, and will generate high frequency output for years 1950 through 2005.  CAM will output data every six hours instead of once a day.  Also starting at year 1950 additional fields will be output by the model.</para>
<orderedlist>
<listitem>
<para> 
The first step is to create case b40.20th.1deg and run the case for years 1850 through 1949 with your initial settings for output. 
</para> 
</listitem>

<listitem>
<para>
Next move your entire case directory, $CASEDIR, somewhere else, because you need to rebuild and rerun the case using the same name.
</para>
<screen>
> cd $CASEDIR 
> mv b40.20th.1deg b40.20th.1deg.1850-1949
</screen>
</listitem>

<listitem>
<para>
Now move your run directory, $RUNDIR, somewhere else as well.  
</para>
<screen>
> cd $RUNDIR 
> mv b40.20th.1deg b40.20th.1deg.1850-1949
</screen>
</listitem>

<listitem>
<para>
Next create a new case in your case directory with the same name, b40.20th.1deg. 
</para>
<screen>
> cd $CASEDIR/scripts
> create_newcase -mach bluefire -compset B_1850-2000_CN -res f09_g16 -case b40.20th.1deg 
cd $RUNDIR
</screen>
</listitem>

<listitem>
<para>
Next edit the namelist file, env_conf.xml, in the run directory, $RUNDIR, as follows: 
</para>
<screen>
> cd $RUNDIR
> xmlchange -file env_conf.xml -id RUN_TYPE -val 'branch'
> xmlchange -file env_conf.xml -id RUN_REFCASE -val 'b40.20th.1deg'
> xmlchange -file env_conf.xml -id RUN_REFDATE -val '1948-01-01'
> xmlchange -file env_conf.xml -id CAM_NML_USE_CASE -val '1850-2005_cam4'
> xmlchange -file env_conf.xml -id BRNCH_RETAIN_CASENAME -val 'TRUE'
> xmlchange -file env_conf.xml -id GET_REFCASE -val 'FALSE'
</screen>
</listitem>

<listitem> 
<para>
Next configure the case and edit the coupler and CAM namelists.
<orderedlist numeration="loweralpha" > 

  <listitem>
  <para> Configure case.</para>
  <screen>
   > configure -case
  </screen>
  </listitem>

  <listitem>
  <para>
   Edit Buildconf/cpl.buildnml.csh.  Replace existing brnch_retain_casename line with the following line
   brnch_retain_casename = .true.
  </para>

  <para>  Edit Buildconf/cam.buildnml.csh.  Check that bndtvghg = '$DIN_LOC_ROOT' and add:</para> 
 
  <screen>&amp;cam_inparm 
       doisccp = .true.        
       isccpdata = '/fis/cgd/cseg/csm/inputdata/atm/cam/rad/isccp.tautab_invtau.nc'        
       mfilt   = 1,365,30,120,240        
       nhtfrq  = 0,-24,-24,-6,-3        
       fincl2  = 'TREFHTMN','TREFHTMX','TREFHT','PRECC','PRECL','PSL'        
       fincl3  = 'CLDICE','CLDLIQ','CLDTOT','CLOUD','CMFMC','CMFMCDZM','FISCCP1',        
                 'FLDS','FLDSC','FLNS','FLUT','FLUTC','FSDS','FSDSC','FSNS',        
                 'FSNSC','FSNTOA','FSNTOAC','LHFLX','OMEGA','OMEGA500',         
                 'PRECSC','PRECSL','PS','Q','QREFHT','RELHUM','RHREFHT','SHFLX',        
                 'SOLIN','T','TGCLDIWP','TGCLDLWP','U','V','Z3'        
       fincl4  = 'PS:I','PSL:I','Q:I','T:I','U:I','V:I','Z3:I'        
       fincl5  = 'CLDTOT','FLDS','FLDSC','FLNS','FLNSC','FSDS','FSDSC','FSNS',        
                 'LHFLX','PRECC','PRECL','PRECSC','PRECSL','SHFLX',        
                 'PS:I','QREFHT:I','TREFHT:I','TS:I'        
                  /
   </screen> 
   </listitem>
</orderedlist>
</para>
</listitem>
</orderedlist>
<orderedlist numeration="arabic" continuation="continues">

<listitem>
<para>
Now build and run the case.
</para>
<screen>
> b40.20th.1deg.$MACH.build
> bsub < b40.20th.1deg.$MACH.run
</screen>
</listitem>

</orderedlist>
</sect1>

</chapter>
